# -*- coding: utf-8 -*-
"""
Created on Mon Feb 27 13:18:32 2023

### check whether the proj weight is necessary
@author: wenzt
"""

import numpy as np
import os
import torch

from dataset_model import FeasDataset, ImageFolderWithIndex, MLP, get_augmentation, get_dataset, get_network
from utils import train_mlp, evaluation, get_output_emb, train_freeze_mlp, train_fine_tune

import argparse
import shutil

from fig_feas import plot_feas_w_lbl

parser = argparse.ArgumentParser(description='')
parser.add_argument('--sampling_strategy', default='random', type=str,
                    help='Sampling strategy')
parser.add_argument('--al_budget', default='[1000]*1', type=str,#[40]*5#[20]*5+[100]*4+[500]*3#[200]*5+[500]*2#
                    help='dims of classifier')
parser.add_argument('--expid', default='test', type=str,
                    help='order of exps')
parser.add_argument('--outpath_base', default='C:\\document\\code\\fullcover_al\\results\\', type=str,
                    help='path of results')
parser.add_argument('--outpath', default=None, type=str,
                    help='path of final exp results, generated by code')
parser.add_argument('--dataset_name', default='imagenet100', type=str,
                    help='name of dataset [imagenet, feas, cifar10, imagenet100, pets, vic_cape_howe, cifar10LT]')
parser.add_argument('--dataset_path', default='C:\\document\\data\\imagenet\\', type=str,
                    help='for fine-tune and freezing & mlp')
parser.add_argument('--img100_subfile', default='C:\\document\\data\\imagenet\\imagenet100.txt', type=str,
                    help='imagenet-100 subfile path')
parser.add_argument('--selfmodel_path', default='C:\\document\\code\\AS4L\\pretrain_encoder\\resnet50_byol_imagenet2012.pth.tar', type=str,##clip pretrained model doesnt need write the model name, just folder path #
                    help='path of selfsup model')
parser.add_argument('--trainidx', default=None, type=str,
                    help='trainidx for vic_cape_howe dataset')
parser.add_argument('--testidx', default=None, type=str,
                    help='testidx for vic_cape_howe dataset')
parser.add_argument('--imb_factor', default=0.1, type=float,
                    help='imbalance factor for imb dataset')
parser.add_argument('--all_imbsel_idx', default='C:\\document\\code\\fullcover_al\\results\\cifar10LT\\all_imb_idx\\c10imb10\\allidx_imb1.npy', type=str,#None
                    help='path for all imb idx generated, for reproducing')

parser.add_argument('--totfeas_path', default='C://document//code//AS4L//pretrain_encoder//totfeas_img100_byol.npy', type=str,# C:\\document\\data\\sq\\test\\uuid\\totfeas_uuid_r50x2_454_455_456.npy
                    help='path of selfsup feas')
parser.add_argument('--totlabel_path', default='C://document//data//imagenet100//labels.npy', type=str,#'C://document//code//AS4L//pretrain_encoder//totlabel.npy'
                    help='path of trainset label')
# parser.add_argument('--clsstd_path', default='C:\\document\\code\\MLP_proxy\\std_imagenet_byoleman.npy', type=str,
#                     help='path of cls std')
parser.add_argument('--factor_std', default=0.25, type=float,#None
                    help='factor of cls std')
parser.add_argument('--clsstd_path', default=None, type=str,#None
                    help='path of cls std')
parser.add_argument('--totfeas_test_path', default='C://document//code//AS4L//pretrain_encoder//totfeas_img100_byol_test.npy', type=str,#'c10/totfeas_byoleman_test.npy'
                    help='path of selfsup feas testset')
parser.add_argument('--totlabel_test_path', default='C://document//data//imagenet100//totlabels_test.npy', type=str,#'C://document//code//AS4L//pretrain_encoder//totlabel_test.npy'
                    help='path of testset label')
parser.add_argument('--load_proj_weight', default=1, type=int,
                    help='initialized classifier weights from projector')
parser.add_argument('--load_al_weight', default=False, type=bool,
                    help='initialized classifier weights from last Active learning round')

parser.add_argument('--feas_norm', default=1, type=int,
                    help='L2 norm feature')

parser.add_argument('--train_eps', default=100, type=int,#50 for imagenet, 100 for others
                    help='# of training epoch')
parser.add_argument('--lr', default=0.1, type=float,#0.05
                    help='learning rate')
parser.add_argument('--cls_lr', default=0.1, type=float,#0.05
                    help='learning rate for classifier')
parser.add_argument('--momentum', default=0.9, type=float,
                    help='momentum')
parser.add_argument('--weight_decay', default=0., type=float,#0003
                    help='weight_decay')
parser.add_argument('--nesterov', default=True, type=bool,
                    help='nesterov')
parser.add_argument('--milestone', default='60, 80', type=str,#'120, 160'
                    help='learning rate schedule (when to drop lr by a ratio)')
parser.add_argument('--early_stop', default=100, type=int,
                    help='efficient AL baseline, early stop')
parser.add_argument('--freezelr', default=0.5, type=float,
                    help='lr in freeze stage')
parser.add_argument('--freeze_eps', default=10, type=int,
                    help='training eps of lp stage')
parser.add_argument('--ft_eps', default=120, type=int,
                    help='training eps of ft stage')

parser.add_argument('--network', default='res50', type=str,
                    help='[res18,res50,res50x2,res50x4,wrn288,clip_ViT-B/32,clip_ViT-B/16]')

parser.add_argument('--batchsize_train', default=512, type=int,#64
                    help='path of testset label')
parser.add_argument('--grad_accu', default=1, type=int,#4
                    help='num grad accum')
parser.add_argument('--batchsize_al_forward', default=512*1, type=int,##128
                    help='path of testset label')
parser.add_argument('--batchsize_evaluation', default=512*1, type=int,#128
                    help='path of testset label')
parser.add_argument('--classifier_dim', default='2048,4096,100', type=str,
                    help='dims of classifier')

parser.add_argument('--training_mode', default=1, type=int,
                    help='0:MLP_proxy(ours), 1:freezing encoder and training classifier, 2:Fine-tuning, 3:LP-FT')
parser.add_argument('--classifier_type', default='MLP', type=str,
                    help='Linear or MLP')

parser.add_argument('--distributed_training', default=False, type=bool,
                    help='using nn.dataparaller')
parser.add_argument('--num_workers', default=0, type=int,#24
                    help='for dataloader')

parser.add_argument('--num_subset', default=320000, type=int,
                    help='AL, badge_partition_subset, the size of subset')
parser.add_argument('--num_partition', default=20, type=int,
                    help='AL, badge_partition_subset, the size of subset')

parser.add_argument('--alidx_path', default=None, type=str,
                    help='if a lblset is given')
parser.add_argument('--alidx_length', default=0, type=int,
                    help='how long the lblset is used in this AL pass')


### resume al
parser.add_argument('--resume_ckpt_path', default=None, type=str,
                    help='path of resume checkpoints')
parser.add_argument('--al_from_sampling', default=1, type=int,
                    help='al loop, for given an initial pool or resume, 1: sampling->training->sampling, 0: training->sampling->training')


### for semi-sup
parser.add_argument('--p_cutoff', default=0.95, type=float,
                    help='threshold for pseudo-labels')
parser.add_argument('--ulb_loss_ratio', default=1, type=int,
                    help='ratio for consistent loss')
parser.add_argument('--uratio', default=1, type=int,
                    help='ratio for unlabeled data wrt labeled data in each batch')

### for ActiveFT 
parser.add_argument('--activeft_temperature', default=0.07, type=float, help='temperature for softmax')
parser.add_argument('--activeft_max_iter', default=100, type=int, help='max iterations')#300
parser.add_argument('--activeft_lr', default=0.001, type=float, help='learning rate')
parser.add_argument('--activeft_init', default='random', type=str, choices=['random', 'fps'])
parser.add_argument('--activeft_distance', default='euclidean', type=str, help='euclidean or cosine')
parser.add_argument('--activeft_scheduler', default='none', type=str, help='scheduler')
parser.add_argument('--activeft_balance', default=1.0, type=float, help='balance ratio')
parser.add_argument('--activeft_batch_size', default=20000, type=int, help='batch size for SGD')
parser.add_argument('--activeft_slice', default=10, type=int, help='size of slice to save memory')


### for ntkcpl
parser.add_argument('--max_cluster_ntkcpl', default=100, type=int, help='max cluster number')#300
parser.add_argument('--subset_ntk', default=0, type=int, help='only using subset for ntkcpl')
parser.add_argument('--subset_size', default=50000, type=int, help='size of subset for ntkcpl')
parser.add_argument('--kmpl', default=1, type=int, help='using kmeans as pseudo-labels')

### for typiclust
parser.add_argument('--max_cluster_typiclust', default=500, type=int, help='max cluster number')#300

### for linearcpl
parser.add_argument('--lambda_reg', default=0.001, type=float, help='max cluster number')#300

parser.add_argument('--hyperalidx', default=None, type=str, help='max cluster number')#300


args = parser.parse_args()

if args.feas_norm == 1:
    args.feas_norm = True
else:
    args.feas_norm = False
    
if args.subset_ntk == 1:
    args.subset_ntk = True
else:
    args.subset_ntk = False

if args.training_mode == 3:
    ftlr = args.lr
    clslr = args.cls_lr

if args.trainidx is not None:
    args.trainidx = np.load(args.trainidx)
if args.testidx is not None:
    args.testidx = np.load(args.testidx)

args.milestone = args.milestone.split(',')
args.milestone = [int(i) for i in args.milestone]

print(args.lr)
print(args.cls_lr)
print(args.expid)

if args.hyperalidx is not None:#np.load('/home/auv/Documents/ziting/code/trainway/alidxrand.npy')
    hyperalidx = np.load(args.hyperalidx)
else:
    hyperalidx = None

indim_classifier, hiddim_classifier, outdim_classifier = args.classifier_dim.split(',')
indim_classifier, hiddim_classifier, outdim_classifier = int(indim_classifier), [int(hiddim_classifier)], int(outdim_classifier)

#indim_classifier, hiddim_classifier, outdim_classifier = 2048,[512],10#[4096,4096]

num_budget = eval(args.al_budget)#
num_al_itr = len(num_budget)

sampling_strategy = args.sampling_strategy#'badge'#'coreset_self'
expid = args.expid#'1_waug'#1

if sampling_strategy == 'badge' or sampling_strategy ==  'badge_partition' or sampling_strategy ==  'badge_partition_subset':    
    from samplings.sampling_strategy import kmeans_plus
    from utils import get_grad_embedding
    if sampling_strategy == 'badge_partition':
        from samplings.sampling_strategy import kmeans_plus_partition
        
elif sampling_strategy == 'entropy':
    from samplings.sampling_strategy import entropy_sampling

elif sampling_strategy == 'margin':
    from samplings.sampling_strategy import margin_sampling, margin_sampling_model

elif sampling_strategy == 'coreset' or sampling_strategy == 'coreset_self':
    from samplings.sampling_strategy import acquire_new_sample
    if sampling_strategy == 'coreset_self':
        totfeas = np.load(args.totfeas_path)
        if ( args.all_imbsel_idx is not None ) and ( 'LT' in args.dataset_name ):
            tallidx = np.load(args.all_imbsel_idx)
            totfeas = totfeas[tallidx,:]

elif sampling_strategy == 'cluster_margin':
    from samplings.sampling_strategy import cluster_margin_sampling, margin_sampling_model, initial_hac

elif sampling_strategy == 'learningloss':
    from samplings.sampling_strategy import est_loss, learning_loss_sampling
    from utils import train_mlp_wlearningloss
    from dataset_model import LossNet
    lossmodel = None

elif sampling_strategy == 'ActiveFT(al)' or sampling_strategy == 'ActiveFT(self)':
    totfeas = np.load(args.totfeas_path)
    if len(totfeas) > 100000:
        from samplings.ActiveFT_large import ActiveFT_sampling
    else:
        from samplings.ActiveFT import ActiveFT_sampling
    
elif sampling_strategy == 'confidence':
    from samplings.sampling_strategy import confidence_sampling_model

elif sampling_strategy == 'typiclust_self' or sampling_strategy == 'typiclust_al':
    from samplings.sampling_strategy import sampling_typicluster
    totfeas = np.load(args.totfeas_path)
    if ( args.all_imbsel_idx is not None ) and ( 'LT' in args.dataset_name ):
        tallidx = np.load(args.all_imbsel_idx)
        totfeas = totfeas[tallidx,:]
    totfeas = totfeas / np.linalg.norm(totfeas, axis=1, keepdims=True)

elif sampling_strategy == 'probcover':
    from samplings.sampling_strategy import construct_dg, sampling_probcover, construct_dg_2, sampling_probcover_2, est_probth
    thresh = 0.95
    totfeas = np.load(args.totfeas_path)
    if ( args.all_imbsel_idx is not None ) and ( 'LT' in args.dataset_name ):
        tallidx = np.load(args.all_imbsel_idx)
        totfeas = totfeas[tallidx,:]
    totfeas = totfeas / np.linalg.norm(totfeas, axis=1, keepdims=True)#depends on dataset 
    
elif sampling_strategy == 'ntkcpl_al' or sampling_strategy == 'ntkcpl_self':
    from samplings.cluster_pruce import generate_cluster_pl, constrain_kmeans
    from samplings.ntk_al import ntk_ker_f0, select_new_sample, ntk_ker_f0_selfinit
    num_min_cluster = 50
    totfeas = np.load(args.totfeas_path)
    totlabel = np.load(args.totlabel_path)
    if ( args.all_imbsel_idx is not None ) and ( 'LT' in args.dataset_name ):
        tallidx = np.load(args.all_imbsel_idx)
        totfeas = totfeas[tallidx,:]
        totlabel = totlabel[tallidx]
    totfeas_norm = totfeas / np.linalg.norm(totfeas, axis=1, keepdims=True)

elif sampling_strategy == 'lookahead':
    from samplings.look_ahead import select_new_sample#,ntk_ker_f0 
    from samplings.ntk_al import ntk_ker_f0
    import jax
    totfeas = np.load(args.totfeas_path)
    if ( args.all_imbsel_idx is not None ) and ( 'LT' in args.dataset_name ):
        tallidx = np.load(args.all_imbsel_idx)
        totfeas = totfeas[tallidx,:]
        
elif sampling_strategy == 'regression_proxy':
    from samplings.ridge_regression_proxy import ridge_regression_sampling
    totfeas = np.load(args.totfeas_path)
    # totlabel = np.load(args.totlabel_path)
    totfeas_norm = totfeas / np.linalg.norm(totfeas, axis=1, keepdims=True)
    
elif sampling_strategy == 'regression_proxy_cpl':
    from samplings.ridge_regression_proxy import ridge_regression_sampling
    from samplings.cluster_pruce import generate_cluster_pl, constrain_kmeans
    
    totfeas = np.load(args.totfeas_path)
    totlabel = np.load(args.totlabel_path)
    # totlabel = np.load(args.totlabel_path)
    totfeas_norm = totfeas / np.linalg.norm(totfeas, axis=1, keepdims=True)
    num_min_cluster = 50


dataset = args.dataset_name#'imagenet100'#

outpath = os.path.join(args.outpath_base, dataset) #os.path.join('C:\\document\\code\\MLP_proxy\\res\\', dataset) # + exp name
exp_name = dataset + '_' + sampling_strategy + '_exp' + str(expid) + '_training_strategy' + str(args.training_mode)
outpath = os.path.join(outpath, exp_name)
os.makedirs(outpath, exist_ok=True) 
args.outpath = outpath

#record configuration file
shutil.copy(os.path.join('.','train.py'), outpath)

selfmodel_path = args.selfmodel_path#'C:\\document\\code\\AS4L\\pretrain_encoder\\resnet50_byol_imagenet2012.pth.tar'
# selfmodel_path = 'C:\\document\\code\\AS4L\\pretrain_encoder\\simsiam-cifar10-experiment-resnet18_cifar_variant1_0909070238.pth'

# load feas,std
if args.training_mode == 0:
    totfeas = np.load(args.totfeas_path)#np.load('C:\\document\\data\\imagenet100\\totfeas.npy')#np.load('C:\\document\\code\\AS4L\\pretrain_encoder\\totfeas.npy')#
    totlabel = np.load(args.totlabel_path)#np.load('C:\\document\\data\\imagenet100\\labels.npy')#np.load('C:\\document\\code\\AS4L\\pretrain_encoder\\totlabel.npy')#
    if args.clsstd_path is not None:
        std = np.load(args.clsstd_path)#np.load('C:\\document\\data\\imagenet100\\std.npy')#np.load('C:\\document\\data\\imagenet100\\std1000.npy')#None#np.load('C:\\document\\code\\MLP_proxy\\std_cifar10.npy')#np.load('C:\\document\\data\\imagenet100\\std1000.npy')
        std = std * args.factor_std
    else:
        std = None
        
    totfeas_test = np.load(args.totfeas_test_path)#np.load('C:\\document\\data\\imagenet100\\totfeas_test.npy')#np.load('C:\\document\\code\\MLP_proxy\\totfeas_cifar10_test.npy')#
    totlabel_test = np.load(args.totlabel_test_path)#np.load('C:\\document\\data\\imagenet100\\totlabels_test.npy')#np.load('C:\\document\\code\\MLP_proxy\\labels_cifar10_test.npy')#

    allset = FeasDataset(totfeas, totlabel, None)
    testset = FeasDataset(totfeas_test, totlabel_test, None)      
    # allset = FeasDataset(totfeas, args.totlabel_path, None)
    # testset = FeasDataset(totfeas_test, totlabel_test, None) 
else:
    transform_train = get_augmentation(args, train = True)
    transform_test = get_augmentation(args, train = False)
    print(transform_train, transform_test)
    allset = get_dataset(args, transform_test, index = None, train = True )
    testset = get_dataset(args, transform_test, index = None, train = False )
    
all_loader = torch.utils.data.DataLoader(
    allset,
    batch_size = args.batchsize_al_forward,
    num_workers = args.num_workers,
    shuffle = False,
    drop_last = False
)
 
test_loader = torch.utils.data.DataLoader(
    testset,
    batch_size = args.batchsize_evaluation,
    num_workers = args.num_workers,
    shuffle = False,
    drop_last = False
)


totemb = None
totpre = None

if args.classifier_type == 'MLP':
    classifier = MLP(indim_classifier, hiddim_classifier, outdim_classifier)
elif args.classifier_type == 'Linear':
    classifier = torch.nn.Linear(indim_classifier,outdim_classifier)
else:
    raise NotImplementedError
    
totacc = []
tracc = []

###load model and initiliaze with self-sup weight 

model = get_network(args)
if ( args.training_mode != 0 ) and ( 'clip' not in args.network ):
    checkpoint = torch.load(selfmodel_path, map_location=torch.device('cpu'))
    #print(model)
    encoder_dict = model.state_dict()
    if args.network == 'res50':
        state_dict = {k[7:]:v for k,v in checkpoint['online_backbone'].items() if k[7:] in encoder_dict.keys()}#byol
        # state_dict = {k[27:]:v for k,v in checkpoint['state_dict'].items() if k[27:] in encoder_dict.keys()}#byol eman
    elif args.network == 'res18' or args.network == 'wrn288':
        if 'simclr' in args.selfmodel_path:
            state_dict = {}
            for k in checkpoint:
                newk = k[9:]
                if 'shortcut' in newk:        
                    newk = newk.replace('shortcut', 'downsample')
                if newk in encoder_dict.keys():
                    state_dict[newk] = checkpoint[k]
        else:
            state_dict = {k[9:]:v for k,v in checkpoint['state_dict'].items() if k[9:] in encoder_dict.keys()} 
        
    else:
        raise NotImplementedError
    encoder_dict.update(state_dict)
    model.load_state_dict(encoder_dict)
    
    model.fc = torch.nn.Identity()  
    
    print('load pretrain ', len(state_dict))
    

### load resume checkpoints
if args.resume_ckpt_path is not None:
    checkpoint_re = torch.load(args.resume_ckpt_path, map_location=torch.device('cpu'))
    # 'module' in state_dict
    # encoder_resume_dict = model.state_dict()
    # state_dict = {k[7:]:v for k,v in checkpoint_re['model_state_dict'].items() if k[7:] in encoder_resume_dict.keys()}#resume
    # encoder_resume_dict.update(state_dict)
    # model.load_state_dict(encoder_resume_dict)
    
    #classifier / model state_dict in 
    model.fc = torch.nn.Identity()   
    model.load_state_dict(checkpoint_re['model_state_dict'])
    classifier.load_state_dict(checkpoint_re['classifier_state_dict'])
    
    model.cuda()
    classifier.cuda()
    
    print('load resume state ', args.resume_ckpt_path)
    

# load part projector weights
print('load classifier head ', args.load_proj_weight)
### byol
if args.load_proj_weight == 1:
    corr = {}
    corr['layer1.0.weight'] = 'l1.weight'
    corr['layer1.0.bias'] = 'l1.bias'
    corr['layer1.1.bias'] = 'bn1.bias'
    corr['layer1.1.weight'] = 'bn1.weight'
    corr['layer1.1.num_batches_tracked'] = 'bn1.num_batches_tracked'
    corr['layer1.1.running_mean'] = 'bn1.running_mean'
    corr['layer1.1.running_var'] = 'bn1.running_var'

    ### byol eman
    # corr = {}
    # corr['layer1.0.weight'] = '0.weight'
    # corr['layer1.1.bias'] = '1.bias'
    # corr['layer1.1.weight'] = '1.weight'
    # corr['layer1.1.num_batches_tracked'] = '1.num_batches_tracked'
    # corr['layer1.1.running_mean'] = '1.running_mean'
    # corr['layer1.1.running_var'] = '1.running_var'

    model_dict = classifier.state_dict()
    state_dict = {k[7:]:v for k,v in checkpoint['online_projection'].items()}
    pre_state_dict = {k[7:]:v for k,v in checkpoint['online_projection'].items()}
    #state_dict = {k[27:]:v for k,v in checkpoint['state_dict'].items() if 'online_net.neck' in k}#byol eman
    state_dict1 = {}
    for ikey in corr:
        state_dict1[ikey] = state_dict[corr[ikey]]
    model_dict.update(state_dict1)  
    print('load classifier head states ', len(state_dict1))

import time

s = time.time()
s0 = time.time()

if args.alidx_path is None:
    alidx = []
    uidx = [i for i in range(len(allset))]
else:
    alidx = np.load(args.alidx_path).tolist()
    alidx = alidx[:args.alidx_length]
    uidx = [i for i in range(len(allset))]
    uidx = list( set(uidx) - set(alidx) )

for alitr in range(num_al_itr):
    
    if (alitr == 0 and args.al_from_sampling) or alitr > 0:#(alitr == 0 and len(alidx) == 0) or (alitr > 0 and len(alidx) > 0):#not resume al or resume al start from retraining
    
        # if args.sampling_strategy != 'coreset' and args.sampling_strategy != 'ActiveFT(al)' and args.sampling_strategy != 'typiclust_al' and sampling_strategy != 'lookahead':
        #     if args.training_mode == 0:
        #         allset = FeasDataset(totfeas[uidx,:], totlabel[uidx], None)     
        #     else:
        #         #allset = ImageFolderWithIndex(root = os.path.join(args.dataset_path, 'train'), indexs= uidx, transform = transform_test)
        #         allset = get_dataset(args, transform_test, index = uidx, train = True )
                
        #     all_loader = torch.utils.data.DataLoader(
        #         allset,
        #         batch_size = args.batchsize_al_forward,
        #         num_workers = args.num_workers,
        #         shuffle = False,
        #         drop_last = False
        #     )
            
    
        if sampling_strategy == 'entropy':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                totpre, _ = get_output_emb(all_loader, classifier, False, args, model)
                # np.save(os.path.join(outpath, 'totpre' + str(alitr) + '.npy'), totpre)
                alidx += entropy_sampling(totpre, uidx, num_budget[alitr])
                uidx = list( set(uidx) - set(alidx) )
                
        elif sampling_strategy == 'random':
            if hyperalidx is not None:
                alidx = hyperalidx[:np.sum(num_budget[:alitr+1])]
                uidx = list( set(uidx) - set(alidx) )
            else:
                np.random.shuffle(uidx)
                # alidx = np.load('C:\\document\\code\\fullcover_al\\alidx.npy')
                # alidx = alidx[:num_budget[alitr]]
                # alidx = [1,2,]
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            
        elif sampling_strategy == 'badge':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                emb = get_grad_embedding(classifier, all_loader, args, model)
                print(len(uidx), len(emb))
                newidx = kmeans_plus(uidx, emb, num_budget[alitr])
                alidx += newidx
                uidx = list( set(uidx) - set(alidx) )
                
        elif sampling_strategy == 'badge_partition':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]]
                uidx = list( set(uidx) - set(alidx) )
            else:
                np.random.shuffle(uidx)
                num_partition = int(len(uidx) / args.num_partition)
                for ipart in range(args.num_partition):
                    print('badge partition ', ipart)
                    allset = FeasDataset( totfeas[uidx[num_partition*ipart:num_partition*(ipart+1)],:], totlabel[uidx[num_partition*ipart:num_partition*(ipart+1)]], None)
                    all_loader = torch.utils.data.DataLoader(
                        allset,
                        batch_size = args.batchsize_al_forward,
                        num_workers = 8,
                        shuffle = False,
                        drop_last = False
                    )
                    emb = get_grad_embedding(classifier, all_loader, args, model)
                    if num_budget[alitr] % args.num_partition != 0:
                        print('wrong partition setting')
                    newidx = kmeans_plus([i for i in range(len(emb))], emb, int(num_budget[alitr]/args.num_partition))
                    alidx += np.array(uidx[num_partition*ipart:num_partition*(ipart+1)])[newidx].tolist()
                    uidx = list( set(uidx) - set(alidx) )
                    
        elif sampling_strategy == 'badge_partition_subset':# set it small to no partition 
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                np.random.shuffle(uidx)
                candidx = uidx[:args.num_subset]
                
                num_partition = int(len(candidx) / args.num_partition)
                for ipart in range(args.num_partition):
                    print('badge partition ', ipart)
                    allset = FeasDataset( totfeas[candidx[num_partition*ipart:num_partition*(ipart+1)],:], totlabel[candidx[num_partition*ipart:num_partition*(ipart+1)]], None)
                    all_loader = torch.utils.data.DataLoader(
                        allset,
                        batch_size = args.batchsize_al_forward,
                        num_workers = 8,
                        shuffle = False,
                        drop_last = False
                    )
                    emb = get_grad_embedding(classifier, all_loader, args, model)
                    if num_budget[alitr] % args.num_partition != 0:
                        print('wrong partition setting')
                    newidx = kmeans_plus([i for i in range(len(emb))], emb, int(num_budget[alitr]/args.num_partition))
                    alidx += np.array(candidx[num_partition*ipart:num_partition*(ipart+1)])[newidx].tolist()
                    uidx = list( set(uidx) - set(alidx) )
                
                
                # allset = FeasDataset( totfeas[candidx,:], totlabel[candidx], None)
                # all_loader = torch.utils.data.DataLoader(
                #     allset,
                #     batch_size = args.batchsize_al_forward,
                #     shuffle = False,
                #     drop_last = False
                # )
                # emb = get_grad_embedding(classifier, all_loader, model) 
                
                # print('grad emb calculated')
                
                # newidx = kmeans_plus([i for i in range(len(candidx))], emb, num_budget[alitr])
                # alidx += np.array(candidx)[newidx].tolist()
                # uidx = list( set(uidx) - set(alidx) )
        
        elif sampling_strategy == 'margin':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                #totpre, _ = get_output_emb(all_loader, classifier, False, model)
                #alidx += margin_sampling(totpre, uidx, num_budget[alitr])
                alidx += margin_sampling_model(uidx, all_loader, classifier, model, num_budget[alitr], args)
                uidx = list( set(uidx) - set(alidx) )
                
        elif sampling_strategy == 'confidence':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                alidx += confidence_sampling_model(uidx, all_loader, classifier, model, num_budget[alitr], args)
                uidx = list( set(uidx) - set(alidx) )
        
        elif sampling_strategy == 'ActiveFT(al)':
            if len(alidx) == 0:
                alidx = ActiveFT_sampling(totfeas, num_budget[alitr], alidx, args)
            else:
                _, alfeas = get_output_emb(all_loader, classifier, True, args, model)
                alidx = ActiveFT_sampling(alfeas, num_budget[alitr], alidx, args)
        
        elif sampling_strategy == 'ActiveFT(self)':
            if len(alidx) == 0:
                alidx = ActiveFT_sampling(totfeas, num_budget[alitr], alidx, args)
            else:
                alidx = alidx = ActiveFT_sampling(totfeas, num_budget[alitr], alidx, args)
        
        elif sampling_strategy == 'coreset':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                _, alfeas = get_output_emb(all_loader, classifier, True, args, model)
                np.save(os.path.join(outpath, 'alfeas' + str(alitr) + '.npy'), alfeas)
                label_feas = alfeas[alidx,:]
                uidx = list( set(uidx) - set(alidx) )
                unlabel_feas = alfeas[uidx,:]
                newidx = acquire_new_sample(num_budget[alitr], uidx, torch.from_numpy(label_feas), torch.from_numpy(unlabel_feas))
                alidx += newidx
        
        elif sampling_strategy == 'coreset_self':
            if len(alidx) == 0:
                alidx = [np.random.randint(0,len(allset))]
            label_feas = totfeas[alidx,:]
            uidx = list( set(uidx) - set(alidx) )
            unlabel_feas = totfeas[uidx,:]
            if len(alidx) == 1:
                newidx = acquire_new_sample(num_budget[alitr]-1, uidx, torch.from_numpy(label_feas), torch.from_numpy(unlabel_feas))
            else:
                newidx = acquire_new_sample(num_budget[alitr], uidx, torch.from_numpy(label_feas), torch.from_numpy(unlabel_feas))
            alidx += newidx
                
        elif sampling_strategy == 'lookahead':# prepare fullnet ntk, f0, or ntk for mlp
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
                
                if (len(totfeas) / 1000) - (len(totfeas) // 1000) != 0:### extend dim of totfeas for fast computation 
                    ext_dim = 1000 - ( len(totfeas) - (len(totfeas) // 1000)*1000 )
                    totfeasext = np.vstack((totfeas, totfeas[:ext_dim,:]))
                    # ker, f0 = ntk_ker_f0(totfeasext, num_classes = outdim_classifier, batch_ker = True)
                    ker, f0 = ntk_ker_f0(totfeasext, num_indim = indim_classifier, num_hidden = hiddim_classifier[0], num_classes = outdim_classifier, scaleidx = None, batch_ker = True)
                    ker = ker[:len(totfeas),:]
                    ker = ker[:,:len(totfeas)]
                    f0 = f0[:len(totfeas)]
                else:
                    ker, f0 = ntk_ker_f0(totfeas, num_indim = indim_classifier, num_hidden = hiddim_classifier[0], num_classes = outdim_classifier, scaleidx = None, batch_ker = True)
                # ker, f0 = ntk_ker_f0(totfeas, num_classes = outdim_classifier, batch_ker = True)
                # backend = jax.lib.xla_bridge.get_backend()
                # for buf in backend.live_buffers(): buf.delete()
                # for buf in backend.live_executables(): buf.delete()
            else:
                totpre, _ = get_output_emb(all_loader, classifier, True, args, model)
                np.save(os.path.join(outpath, 'totpre' + str(alitr) + '.npy'), totpre)
                alidx = select_new_sample(alidx, [ker], [f0], [totpre.argmax(axis=1)], num_budget[alitr], outdim_classifier, outpath, stepsize = 250)
                
        
        elif sampling_strategy == 'cluster_margin':#TODO not fully correct
            if len(alidx) == 0:
                uidx = list( set(uidx) - set(alidx) )
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
                # initial_center, initial_cluster = initial_hac(totfeas[uidx[-num_budget[alitr]*10:],:])
                initial_cluster = initial_hac(totfeas[uidx[-num_budget[alitr]*1:],:])
                initial_center = totfeas[uidx[-num_budget[alitr]*1:]]
            else:
                candidx = margin_sampling_model(uidx, all_loader, classifier, None, num_budget[alitr]*5, args)
                newidx, initial_cluster, initial_center = cluster_margin_sampling(candidx, totfeas[candidx,:], initial_center, initial_cluster, num_budget[alitr])
                alidx += newidx
                uidx = list( set(uidx) - set(alidx) )
                
                
        elif sampling_strategy == 'typiclust_self':
            newidx = sampling_typicluster(totfeas, alidx, num_budget[alitr], args)
            alidx += newidx
        
        elif sampling_strategy == 'typiclust_al':
            if len(alidx) == 0:
                newidx = sampling_typicluster(totfeas, alidx, num_budget[alitr], args)
                alidx += newidx
            else:
                _, alfeas = get_output_emb(all_loader, classifier, True, args, model)
                newidx = sampling_typicluster(alfeas, alidx, num_budget[alitr])
                alidx += newidx
        
        # elif sampling_strategy == 'probcover':# low budget strategy - small scale
        #     if len(alidx) == 0:    
        #         dg = construct_dg(totfeas, outdim_classifier, thresh)
                
        #     newidx = sampling_probcover(dg, num_budget[alitr])
        #     alidx += newidx
            
        elif sampling_strategy == 'probcover':# low budget strategy - small scale
            if len(alidx) == 0:    
                dist_thresh = est_probth(totfeas, outdim_classifier, thresh)
                dg = construct_dg_2(totfeas, outdim_classifier, dist_thresh) #given dist-thresh
                allnewidx = sampling_probcover_2(dg, np.sum(num_budget), len(totfeas) )
                
            alidx = allnewidx[:np.sum(num_budget[:alitr+1])]
        
        
        elif sampling_strategy == 'regression_proxy':
            if len(alidx) == 0:
                if args.feas_norm:
                    cluster,_ = constrain_kmeans(totfeas_norm, [], [], outdim_classifier)
                else:
                    cluster,_ = constrain_kmeans(totfeas, [], [], outdim_classifier)
                np.save(os.path.join(outpath, 'cluster' + str(alitr) + '_itr0.npy'), cluster)
        
            alidx = ridge_regression_sampling(totfeas, cluster, alidx, num_budget[alitr], outpath, args)
            uidx = list( set(uidx) - set(alidx) )
            
        elif sampling_strategy == 'regression_proxy_cpl':
            if len(alidx) == 0:
                if args.feas_norm:
                    cluster,_ = constrain_kmeans(totfeas_norm, [], [], outdim_classifier)
                else:
                    cluster,_ = constrain_kmeans(totfeas, [], [], outdim_classifier)
            
            else:
                totpre, alfeas = get_output_emb(all_loader, classifier, True, args, model = model)
                if args.feas_norm:
                    alfeas = alfeas / np.linalg.norm(alfeas, axis=1, keepdims=True)
                
                num_cluster = int( (len(alidx) + num_budget[alitr]) / 2)#30#
                if num_cluster > args.max_cluster_ntkcpl:
                	num_cluster = args.max_cluster_ntkcpl
                	num_min_cluster = 10#num_cluster = 100
                cluster = generate_cluster_pl(alfeas, alidx, totlabel[alidx], totpre, num_cluster, totlabel, num_level = 14, num_min_cluster = num_min_cluster, num_class = outdim_classifier)
               
            np.save(os.path.join(outpath, 'cluster' + str(alitr) + '.npy'), cluster)
        
            alidx = ridge_regression_sampling(totfeas, cluster, alidx, num_budget[alitr], outpath, args)
            uidx = list( set(uidx) - set(alidx) )
            
            
        elif sampling_strategy == 'ntkcpl_self' or sampling_strategy == 'ntkcpl_al':# prepare totpre, totfeas/alfeas, hyperparams
            if len(alidx) == 0:
                
                num_cluster = num_budget[0]#totfeas_norm
                if args.kmpl:
                    cluster,_ = constrain_kmeans(totfeas_norm, [], [], outdim_classifier)
                else:
                    if args.feas_norm:
                        cluster,_ = constrain_kmeans(totfeas_norm, [], [], num_cluster)
                    else:
                        cluster,_ = constrain_kmeans(totfeas, [], [], num_cluster)
                np.save(os.path.join(outpath, 'cluster' + str(alitr) + '_itr0.npy'), cluster)
                
                if args.subset_ntk:
                    scaleidx = [i for i in range(len(totfeas))]
                    np.random.shuffle(scaleidx)
                    scaleidx = scaleidx[:args.subset_size]
                    np.save(os.path.join(outpath, 'scaleidx_' + str(alitr) + '.npy'), scaleidx)
                    
                    ker, f0 = ntk_ker_f0(totfeas, num_indim = indim_classifier, num_hidden = hiddim_classifier[0], num_classes = num_cluster, scaleidx = scaleidx, batch_ker = True)
                    
                    alidx = select_new_sample([], [ker], [f0], [cluster[scaleidx]], num_budget[alitr], [num_cluster], outpath = outpath, stepsize = 250)
                    
                    alidx = (np.array(scaleidx)[alidx]).tolist()
                
                else:
                    if (len(totfeas) / 1000) - (len(totfeas) // 1000) != 0:### extend dim of totfeas for fast computation 
                        ext_dim = 1000 - ( len(totfeas) - (len(totfeas) // 1000)*1000 )
                        totfeasext = np.vstack((totfeas, totfeas[:ext_dim,:]))
                        ker, f0 = ntk_ker_f0(totfeasext, num_indim = indim_classifier, num_hidden = hiddim_classifier[0], num_classes = args.max_cluster_ntkcpl, scaleidx = None, batch_ker = True)
                        ker = ker[:len(totfeas),:]
                        ker = ker[:,:len(totfeas)]
                        f0 = f0[:len(totfeas)]
                    else:
                        ker, f0 = ntk_ker_f0(totfeas, num_indim = indim_classifier, num_hidden = hiddim_classifier[0], num_classes = args.max_cluster_ntkcpl, scaleidx = None, batch_ker = True)
                    alidx = select_new_sample([], [ker], [f0[:,:num_cluster]], [cluster], num_budget[alitr], [num_cluster], outpath = outpath, stepsize = 250)
                    
                    
                #ker, f0 = ntk_ker_f0(totfeas, num_classes = num_cluster, batch_ker = True)
                #alidx = select_new_sample([], [ker], [f0], [cluster], num_budget[alitr], [num_cluster], outpath = outpath, stepsize = 250)
                
                ### for self-init ntk 
                # ker, f0 = ntk_ker_f0_selfinit(totfeas, num_indim = 2048, num_hidden = 4096, num_classes = num_cluster, scaleidx = scaleidx, batch_ker = True, pre_state_dict = pre_state_dict)
                
            else:
                #prepare for al
                num_cluster = int( (len(alidx) + num_budget[alitr]) / 2)#30#
                if num_cluster > args.max_cluster_ntkcpl:
                	num_cluster = args.max_cluster_ntkcpl
                	num_min_cluster = 10#num_cluster = 100
                
                #totpre, alfeas = get_output_emb(all_loader, model, classifier)
                totpre, alfeas = get_output_emb(all_loader, classifier, True, args, model = model)
                np.save(os.path.join(outpath, 'totpre' + str(alitr) + '.npy'), totpre)   
                
                if args.kmpl == 0:
                    if sampling_strategy == 'ntkcpl_al':
                        if args.feas_norm:
                            alfeas = alfeas / np.linalg.norm(alfeas, axis=1, keepdims=True)
                        cluster = generate_cluster_pl(alfeas, alidx, totlabel[alidx], totpre, num_cluster, totlabel, num_level = 14, num_min_cluster = num_min_cluster, num_class = outdim_classifier)
                    else:
                        if args.feas_norm:
                            cluster = generate_cluster_pl(totfeas_norm, alidx, totlabel[alidx], totpre, num_cluster, totlabel, num_level = 14, num_min_cluster = num_min_cluster, num_class = outdim_classifier)
                        else:
                            cluster = generate_cluster_pl(totfeas, alidx, totlabel[alidx], totpre, num_cluster, totlabel, num_level = 14, num_min_cluster = num_min_cluster, num_class = outdim_classifier)
            
                np.save(os.path.join(outpath, 'cluster' + str(alitr +1) + '_' + str(num_cluster) + '.npy'), cluster)
                
                
                if args.subset_ntk:
                    scaleidx = [i for i in range(len(totfeas)) if i not in alidx]
                    np.random.shuffle(scaleidx)
                    scaleidx = alidx + scaleidx
                    scaleidx = scaleidx[:args.subset_size]#scaleidx[:args.subset_size + args.alidx_length + np.sum(num_budget)]
                    np.save(os.path.join(outpath, 'scaleidx_' + str(alitr) + '.npy'), scaleidx)
                
                    ker, f0 = ntk_ker_f0(totfeas, num_indim = indim_classifier, num_hidden = hiddim_classifier[0], num_classes = num_cluster, scaleidx = scaleidx, batch_ker = True)   
                    
                    if len(alidx) > 1000:
                        stepsize = 100
                    else:
                        stepsize = 250
                    alidx = select_new_sample([i for i in range(len(alidx))], [ker], [f0], [cluster[scaleidx]], num_budget[alitr], [num_cluster], outpath = outpath, stepsize = stepsize)
                    
                    alidx = (np.array(scaleidx)[alidx]).tolist()
                
                else:
                    alidx = select_new_sample(alidx, [ker], [f0[:,:num_cluster]], [cluster], num_budget[alitr], [num_cluster], outpath = outpath, stepsize = 250)
                    
                ### al
                # ker, f0 = ntk_ker_f0(totfeas, num_classes = num_cluster, batch_ker = True)
                #alidx = select_new_sample(alidx, [ker], [f0], [cluster], num_budget[alitr], [num_cluster], outpath = outpath, stepsize = 250)
                
                ### for self-init ntk 
                # ker, f0 = ntk_ker_f0_selfinit(totfeas, num_indim = 2048, num_hidden = 4096, num_classes = num_cluster, scaleidx = scaleidx, batch_ker = True, pre_state_dict = pre_state_dict)


        
        # np.save(os.path.join(outpath, 'alidx' + str(alitr) + '.npy'), np.array(alidx))
        np.save(os.path.join(outpath, 'alidx.npy'), np.array(alidx))
    
        print('point 1 sample selection', time.time() - s)
        s = time.time()
    
    if args.training_mode == 0:
        trainset = FeasDataset(totfeas[alidx,:], totlabel[alidx], std)
    else:
        trainset = get_dataset(args, transform_train, index = alidx, train = True )
    
    if len(trainset) < args.batchsize_train:
        Droplast = False
    else:
        Droplast = True
    train_loader = torch.utils.data.DataLoader(
        trainset,
        batch_size = args.batchsize_train,
        num_workers = args.num_workers,
        shuffle=True,
        #drop_last = False,#Droplast
    )
    
    ###class weight CE loss for class imbalance dataset
    if 'LTt' in args.dataset_name:# not that useful for freezemlp, just commend out
        lbltarget = trainset.targets
        class_weight = np.bincount(lbltarget, minlength=outdim_classifier )
        class_weight = 1./(class_weight+1)
        class_weight = class_weight / class_weight.sum()
        class_weight = torch.from_numpy(class_weight).float().cuda()
    else:
        class_weight = None
    
    if (args.load_al_weight and alitr == 0) or (not args.load_al_weight):
        if args.classifier_type == 'MLP':
            classifier = MLP(indim_classifier, hiddim_classifier, outdim_classifier)
        elif args.classifier_type == 'Linear':
            classifier = torch.nn.Linear(indim_classifier,outdim_classifier)
        else:
            raise NotImplementedError
    
    if args.load_proj_weight == 1:
        classifier.load_state_dict(model_dict)
    
    classifier.cuda()
    if args.distributed_training:
        classifier = torch.nn.DataParallel(classifier)
    
    if ( args.training_mode != 0 ) and ( 'clip' not in args.network ): 
        model = get_network(args)
        model.load_state_dict(encoder_dict)
        model.fc = torch.nn.Identity() 
        model.cuda()
        if args.distributed_training:
            model = torch.nn.DataParallel(model)
    
    print('point 2 model load', time.time() - s)
    s = time.time()
    
    ### training
    if args.training_mode == 0: 
        classifier, trainloss = train_mlp(train_loader, classifier, args, class_weight = class_weight)
        # torch.save({'epoch': 100, 'state_dict': classifier.state_dict()}, os.path.join(outpath, 'classifier' + str(len(alidx)) + '.pth'))
        torch.save({'epoch': args.train_eps, 'state_dict': classifier.state_dict()}, os.path.join(outpath, 'classifier.pth'))
    elif args.training_mode == 1:
        classifier, trainloss = train_freeze_mlp(train_loader, model, classifier, args, class_weight = class_weight)
        torch.save({'epoch': args.train_eps, 'classifier_state_dict': classifier.state_dict()}, os.path.join(outpath, 'checkpoint_' + str(len(alidx)) + '_.pth.tar'))
    elif args.training_mode == 2: 
        model, classifier, trainloss = train_fine_tune(train_loader, model, classifier, args, class_weight = class_weight)
        torch.save({'epoch': args.train_eps, 'classifier_state_dict': classifier.state_dict(), 'model_state_dict': model.state_dict()}, os.path.join(outpath, 'checkpoint_' + str(len(alidx)) + '_.pth.tar'))
    elif args.training_mode == 3:
        ### LP stage
        args.cls_lr = args.freezelr
        args.train_eps = args.freeze_eps
        classifier, trainloss = train_freeze_mlp(train_loader, model, classifier, args, class_weight = class_weight)# train_mlp(train_loader, classifier, args)
        print('trainloss freeze lp ', trainloss)
        tacc = evaluation(test_loader, classifier, model = model)
        torch.save({'acc': tacc, 'classifier_state_dict': classifier.state_dict()}, os.path.join(outpath, 'classifier_' + str(len(alidx)) + '.pth'))
        
        # FT stage
        args.lr = ftlr
        args.cls_lr = clslr
        args.train_eps = args.ft_eps
        model, classifier, trainloss = train_fine_tune(train_loader, model, classifier, args, class_weight = class_weight)
        print('trainloss ft ', trainloss)
        torch.save({'epoch': args.train_eps, 'classifier_state_dict': classifier.state_dict(), 'model_state_dict': model.state_dict()}, os.path.join(outpath, 'checkpoint_' + str(len(alidx)) + '_.pth.tar'))
        
    else:
        raise NotImplementedError
    
    print('point 3 training', time.time() - s)
    s = time.time()
    
    ### evaluation
    # testset = FeasDataset(totfeas_test[:,:], totlabel_test[:], None)
    # test_loader = torch.utils.data.DataLoader(
    #     testset,
    #     batch_size = args.batchsize_evaluation,
    #     num_workers = args.num_workers,
    #     shuffle = False,
    #     drop_last = False
    # )
    
    acc = evaluation(test_loader, classifier, model = model)
    tacc = evaluation(train_loader, classifier, model = model)
    
    #visualization
    # totpre, _ = get_output_emb(all_loader, classifier, True, args, model)
    # totpre = totpre.argmax(axis=1)
    # X_embed_wrn = np.load('C://document//code//AS4L//pretrain_encoder//X_embed_wrn.npy')
    # plot_feas_w_lbl(X_embed_wrn, totpre, classname = None, lblidx = alidx, outputpath= os.path.join(outpath, 'lbl' + str(len(alidx)))  )
    
    print('point 4 evaluation', time.time() - s)
    s = time.time()
    
    #np.save(outpath + 'totpre' + str(len(alidx)) + '.npy', totpre)
    #np.save(outpath + 'alfeas' + str(len(alidx)) + '.npy', totemb)
    totacc += [acc]
    tracc += [tacc]
    print('AL lblset size is ', len(alidx), 'time ', time.time() - s)
    s = time.time()
    print('test acc: ', acc)
    print('train acc: ', tacc)
    np.save(os.path.join(outpath, 'acc.npy'), np.array(totacc))

### save
np.save(os.path.join(outpath, 'acc.npy'), np.array(totacc))

print('total time:', time.time() - s0)
