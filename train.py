# -*- coding: utf-8 -*-
"""
Created on Mon Feb 27 13:18:32 2023

@author: wenzt
"""

import numpy as np
import os
import torch

from dataset_model import FeasDataset, ImageFolderWithIndex, MLP, get_augmentation, get_dataset, get_network
from utils import train_mlp, evaluation, get_output_emb, train_freeze_mlp, train_fine_tune

import argparse
import shutil

parser = argparse.ArgumentParser(description='')
parser.add_argument('--sampling_strategy', default='typiclust_self', type=str,
                    help='Sampling strategy')
parser.add_argument('--al_budget', default='[200]*10', type=str,
                    help='dims of classifier')
parser.add_argument('--expid', default='1_test', type=str,
                    help='order of exps')
parser.add_argument('--outpath_base', default='./results', type=str,
                    help='path of results')
parser.add_argument('--outpath', default=None, type=str,
                    help='path of final exp results, generated by code')
parser.add_argument('--dataset_name', default='cifar10', type=str,
                    help='name of dataset [imagenet, feas, cifar10, imagenet100, pets, vic_cape_howe, cifar10LT]')
parser.add_argument('--dataset_path', default='./data/cifar10', type=str,
                    help='for fine-tune and freezing & mlp')
parser.add_argument('--img100_subfile', default='./data/imagenet/imagenet100.txt', type=str,
                    help='imagenet-100 subfile path')
parser.add_argument('--selfmodel_path', default='./pretrain_encoder/simsiam-cifar10-experiment-resnet18_cifar_variant1_0909070238.pth', type=str,##clip pretrained model doesnt need write the model name, just folder path #
                    help='path of selfsup model')
parser.add_argument('--trainidx', default=None, type=str,
                    help='trainidx for vic_cape_howe dataset')
parser.add_argument('--testidx', default=None, type=str,
                    help='testidx for vic_cape_howe dataset')
parser.add_argument('--imb_factor', default=0.1, type=float,
                    help='imbalance factor for imb dataset')
parser.add_argument('--all_imbsel_idx', default='./cifar10LT/all_imb_idx/c10imb10/allidx_imb1.npy', type=str,#None
                    help='path for all imb idx generated, for reproducing')

parser.add_argument('--totfeas_path', default='./pretrain_encoder/totfeas.npy', type=str,#'c10/totfeas_byoleman.npy'
                    help='path of selfsup feas')
parser.add_argument('--totlabel_path', default='./pretrain_encoder/label.npy', type=str,
                    help='path of trainset label')
# parser.add_argument('--clsstd_path', default='C:\\document\\code\\MLP_proxy\\std_imagenet_byoleman.npy', type=str,
#                     help='path of cls std')
parser.add_argument('--factor_std', default=0.25, type=float,#None
                    help='factor of cls std')
parser.add_argument('--clsstd_path', default=None, type=str,#None
                    help='path of cls std')
parser.add_argument('--totfeas_test_path', default='./pretrain_encoder/totfeas_test.npy', type=str,#'c10/totfeas_byoleman_test.npy'
                    help='path of selfsup feas testset')
parser.add_argument('--totlabel_test_path', default='./pretrain_encoder/label_test.npy', type=str,
                    help='path of testset label')
parser.add_argument('--load_proj_weight', default=0, type=int,
                    help='initialized classifier weights from projector')
parser.add_argument('--load_al_weight', default=False, type=bool,
                    help='initialized classifier weights from last Active learning round')

parser.add_argument('--feas_norm', default=1, type=int,
                    help='L2 norm feature')

parser.add_argument('--train_eps', default=100, type=int,#50 for imagenet, 100 for others
                    help='# of training epoch')
parser.add_argument('--lr', default=0.3, type=float,#0.05
                    help='learning rate')
parser.add_argument('--cls_lr', default=0.3, type=float,#0.05
                    help='learning rate for classifier')
parser.add_argument('--momentum', default=0.9, type=float,
                    help='momentum')
parser.add_argument('--weight_decay', default=0.0003, type=float,#0003
                    help='weight_decay')
parser.add_argument('--nesterov', default=True, type=bool,
                    help='nesterov')
parser.add_argument('--milestone', default='60, 80', type=str,#'120, 160'
                    help='learning rate schedule (when to drop lr by a ratio)')
parser.add_argument('--early_stop', default=100, type=int,
                    help='efficient AL baseline, early stop')
parser.add_argument('--freezelr', default=0.5, type=float,
                    help='lr in freeze stage')
parser.add_argument('--freeze_eps', default=10, type=int,
                    help='training eps of lp stage')
parser.add_argument('--ft_eps', default=120, type=int,
                    help='training eps of ft stage')

parser.add_argument('--network', default='res18', type=str,
                    help='[res18,res50,res50x2,res50x4,wrn288,clip_ViT-B/32,clip_ViT-B/16]')

parser.add_argument('--batchsize_train', default=128, type=int,#64
                    help='path of testset label')
parser.add_argument('--grad_accu', default=1, type=int,#4
                    help='num grad accum')
parser.add_argument('--batchsize_al_forward', default=256*1, type=int,##128
                    help='path of testset label')
parser.add_argument('--batchsize_evaluation', default=256*1, type=int,#128
                    help='path of testset label')
parser.add_argument('--classifier_dim', default='512,512,10', type=str,
                    help='dims of classifier')

parser.add_argument('--training_mode', default=1, type=int,
                    help='0:MLP_proxy(ours), 1:freezing encoder and training classifier, 2:Fine-tuning, 3:LP-FT')
parser.add_argument('--classifier_type', default='MLP', type=str,
                    help='Linear or MLP')

parser.add_argument('--distributed_training', default=False, type=bool,
                    help='using nn.dataparaller')
parser.add_argument('--num_workers', default=0, type=int,#24
                    help='for dataloader')

parser.add_argument('--num_subset', default=320000, type=int,
                    help='AL, badge_partition_subset, the size of subset')
parser.add_argument('--num_partition', default=20, type=int,
                    help='AL, badge_partition_subset, the size of subset')

parser.add_argument('--alidx_path', default=None, type=str,
                    help='if a lblset is given')
parser.add_argument('--alidx_length', default=0, type=int,
                    help='how long the lblset is used in this AL pass')


### resume al
parser.add_argument('--resume_ckpt_path', default=None, type=str,
                    help='path of resume checkpoints')
parser.add_argument('--al_from_sampling', default=1, type=int,
                    help='al loop, for given an initial pool or resume, 1: sampling->training->sampling, 0: training->sampling->training')


### for semi-sup
parser.add_argument('--p_cutoff', default=0.95, type=float,
                    help='threshold for pseudo-labels')
parser.add_argument('--ulb_loss_ratio', default=1, type=int,
                    help='ratio for consistent loss')
parser.add_argument('--uratio', default=1, type=int,
                    help='ratio for unlabeled data wrt labeled data in each batch')

### for ActiveFT 
parser.add_argument('--activeft_temperature', default=0.07, type=float, help='temperature for softmax')
parser.add_argument('--activeft_max_iter', default=100, type=int, help='max iterations')#300
parser.add_argument('--activeft_lr', default=0.001, type=float, help='learning rate')
parser.add_argument('--activeft_init', default='random', type=str, choices=['random', 'fps'])
parser.add_argument('--activeft_distance', default='euclidean', type=str, help='euclidean or cosine')
parser.add_argument('--activeft_scheduler', default='none', type=str, help='scheduler')
parser.add_argument('--activeft_balance', default=1.0, type=float, help='balance ratio')
parser.add_argument('--activeft_batch_size', default=20000, type=int, help='batch size for SGD')
parser.add_argument('--activeft_slice', default=10, type=int, help='size of slice to save memory')


parser.add_argument('--hyperalidx', default=None, type=str, help='max cluster number')#300


args = parser.parse_args()

if args.feas_norm == 1:
    args.feas_norm = True
else:
    args.feas_norm = False
    
if args.subset_ntk == 1:
    args.subset_ntk = True
else:
    args.subset_ntk = False

if args.training_mode == 3:
    ftlr = args.lr
    clslr = args.cls_lr

if args.trainidx is not None:
    args.trainidx = np.load(args.trainidx)
if args.testidx is not None:
    args.testidx = np.load(args.testidx)

args.milestone = args.milestone.split(',')
args.milestone = [int(i) for i in args.milestone]

print(args.lr)
print(args.cls_lr)
print(args.expid)

if args.hyperalidx is not None:#np.load('/home/auv/Documents/ziting/code/trainway/alidxrand.npy')
    hyperalidx = np.load(args.hyperalidx)
else:
    hyperalidx = None

indim_classifier, hiddim_classifier, outdim_classifier = args.classifier_dim.split(',')
indim_classifier, hiddim_classifier, outdim_classifier = int(indim_classifier), [int(hiddim_classifier)], int(outdim_classifier)

#indim_classifier, hiddim_classifier, outdim_classifier = 2048,[512],10#[4096,4096]

num_budget = eval(args.al_budget)#
num_al_itr = len(num_budget)

sampling_strategy = args.sampling_strategy#'badge'#'coreset_self'
expid = args.expid#'1_waug'#1

if sampling_strategy == 'badge' or sampling_strategy ==  'badge_partition' or sampling_strategy ==  'badge_partition_subset':    
    from samplings.sampling_strategy import kmeans_plus
    from utils import get_grad_embedding
    if sampling_strategy == 'badge_partition':
        from samplings.sampling_strategy import kmeans_plus_partition
        
elif sampling_strategy == 'entropy':
    from samplings.sampling_strategy import entropy_sampling

elif sampling_strategy == 'margin':
    from samplings.sampling_strategy import margin_sampling, margin_sampling_model

elif sampling_strategy == 'coreset' or sampling_strategy == 'coreset_self':
    from samplings.sampling_strategy import acquire_new_sample
    if sampling_strategy == 'coreset_self':
        totfeas = np.load(args.totfeas_path)
        if ( args.all_imbsel_idx is not None ) and ( 'LT' in args.dataset_name ):
            tallidx = np.load(args.all_imbsel_idx)
            totfeas = totfeas[tallidx,:]

elif sampling_strategy == 'ActiveFT(al)' or sampling_strategy == 'ActiveFT(self)':
    totfeas = np.load(args.totfeas_path)
    if len(totfeas) > 100000:
        from samplings.ActiveFT_large import ActiveFT_sampling
    else:
        from samplings.ActiveFT_large import ActiveFT_sampling#from samplings.ActiveFT import ActiveFT_sampling
    
elif sampling_strategy == 'confidence':
    from samplings.sampling_strategy import confidence_sampling_model


dataset = args.dataset_name#'imagenet100'#

outpath = os.path.join(args.outpath_base, dataset)
exp_name = dataset + '_' + sampling_strategy + '_exp' + str(expid) + '_training_strategy' + str(args.training_mode)
outpath = os.path.join(outpath, exp_name)
os.makedirs(outpath, exist_ok=True) 
args.outpath = outpath

#record configuration file
shutil.copy(os.path.join('.','train.py'), outpath)

selfmodel_path = args.selfmodel_path

# load feas,std
if args.training_mode == 0:
    totfeas = np.load(args.totfeas_path)#
    totlabel = np.load(args.totlabel_path)#
    if args.clsstd_path is not None:
        std = np.load(args.clsstd_path)#
        std = std * args.factor_std
    else:
        std = None
        
    totfeas_test = np.load(args.totfeas_test_path)#
    totlabel_test = np.load(args.totlabel_test_path)#

    allset = FeasDataset(totfeas, totlabel, None)
    testset = FeasDataset(totfeas_test, totlabel_test, None)    
else:
    transform_train = get_augmentation(args, train = True)
    transform_test = get_augmentation(args, train = False)
    print(transform_train, transform_test)
    allset = get_dataset(args, transform_test, index = None, train = True )
    testset = get_dataset(args, transform_test, index = None, train = False )
    
all_loader = torch.utils.data.DataLoader(
    allset,
    batch_size = args.batchsize_al_forward,
    num_workers = args.num_workers,
    shuffle = False,
    drop_last = False
)
 
test_loader = torch.utils.data.DataLoader(
    testset,
    batch_size = args.batchsize_evaluation,
    num_workers = args.num_workers,
    shuffle = False,
    drop_last = False
)


totemb = None
totpre = None

if args.classifier_type == 'MLP':
    classifier = MLP(indim_classifier, hiddim_classifier, outdim_classifier)
elif args.classifier_type == 'Linear':
    classifier = torch.nn.Linear(indim_classifier,outdim_classifier)
else:
    raise NotImplementedError
    
totacc = []
tracc = []

###load model and initiliaze with self-sup weight 

model = get_network(args)
if ( args.training_mode != 0 ) and ( 'clip' not in args.network ):
    checkpoint = torch.load(selfmodel_path, map_location=torch.device('cpu'))
    #print(model)
    encoder_dict = model.state_dict()
    if args.network == 'res50':
        # state_dict = {k[7:]:v for k,v in checkpoint['online_backbone'].items() if k[7:] in encoder_dict.keys()}#byol
        state_dict = {k[27:]:v for k,v in checkpoint['state_dict'].items() if k[27:] in encoder_dict.keys()}#byol eman
    elif args.network == 'res18' or args.network == 'wrn288':
        if 'simclr' in args.selfmodel_path:
            state_dict = {}
            for k in checkpoint:
                newk = k[9:]
                if 'shortcut' in newk:        
                    newk = newk.replace('shortcut', 'downsample')
                if newk in encoder_dict.keys():
                    state_dict[newk] = checkpoint[k]
        else:
            state_dict = {k[9:]:v for k,v in checkpoint['state_dict'].items() if k[9:] in encoder_dict.keys()} 
        
    else:
        raise NotImplementedError
    encoder_dict.update(state_dict)
    model.load_state_dict(encoder_dict)
    
    model.fc = torch.nn.Identity()  
    
    print('load pretrain ', len(state_dict))
    

### load resume checkpoints
if args.resume_ckpt_path is not None:
    checkpoint_re = torch.load(args.resume_ckpt_path, map_location=torch.device('cpu'))
    # 'module' in state_dict
    # encoder_resume_dict = model.state_dict()
    # state_dict = {k[7:]:v for k,v in checkpoint_re['model_state_dict'].items() if k[7:] in encoder_resume_dict.keys()}#resume
    # encoder_resume_dict.update(state_dict)
    # model.load_state_dict(encoder_resume_dict)
    
    #classifier / model state_dict in 
    model.fc = torch.nn.Identity()   
    model.load_state_dict(checkpoint_re['model_state_dict'])
    classifier.load_state_dict(checkpoint_re['classifier_state_dict'])
    
    model.cuda()
    classifier.cuda()
    
    print('load resume state ', args.resume_ckpt_path)
    

# load part projector weights
print('load classifier head ', args.load_proj_weight)
### byol
if args.load_proj_weight == 1:
    corr = {}
    corr['layer1.0.weight'] = 'l1.weight'
    corr['layer1.0.bias'] = 'l1.bias'
    corr['layer1.1.bias'] = 'bn1.bias'
    corr['layer1.1.weight'] = 'bn1.weight'
    corr['layer1.1.num_batches_tracked'] = 'bn1.num_batches_tracked'
    corr['layer1.1.running_mean'] = 'bn1.running_mean'
    corr['layer1.1.running_var'] = 'bn1.running_var'

    ### byol eman
    # corr = {}
    # corr['layer1.0.weight'] = '0.weight'
    # corr['layer1.1.bias'] = '1.bias'
    # corr['layer1.1.weight'] = '1.weight'
    # corr['layer1.1.num_batches_tracked'] = '1.num_batches_tracked'
    # corr['layer1.1.running_mean'] = '1.running_mean'
    # corr['layer1.1.running_var'] = '1.running_var'

    model_dict = classifier.state_dict()
    state_dict = {k[7:]:v for k,v in checkpoint['online_projection'].items()}
    pre_state_dict = {k[7:]:v for k,v in checkpoint['online_projection'].items()}
    #state_dict = {k[27:]:v for k,v in checkpoint['state_dict'].items() if 'online_net.neck' in k}#byol eman
    state_dict1 = {}
    for ikey in corr:
        state_dict1[ikey] = state_dict[corr[ikey]]
    model_dict.update(state_dict1)  
    print('load classifier head states ', len(state_dict1))

import time

s = time.time()
s0 = time.time()

if args.alidx_path is None:
    alidx = []
    uidx = [i for i in range(len(allset))]
else:
    alidx = np.load(args.alidx_path).tolist()
    alidx = alidx[:args.alidx_length]
    uidx = [i for i in range(len(allset))]
    uidx = list( set(uidx) - set(alidx) )

for alitr in range(num_al_itr):
    
    if (alitr == 0 and args.al_from_sampling) or alitr > 0:#(alitr == 0 and len(alidx) == 0) or (alitr > 0 and len(alidx) > 0):#not resume al or resume al start from retraining
    
        if args.sampling_strategy != 'coreset' and args.sampling_strategy != 'ActiveFT(al)':
            if args.training_mode == 0:
                allset = FeasDataset(totfeas[uidx,:], totlabel[uidx], None)     
            else:
                allset = get_dataset(args, transform_test, index = uidx, train = True )
                
            all_loader = torch.utils.data.DataLoader(
                allset,
                batch_size = args.batchsize_al_forward,
                num_workers = args.num_workers,
                shuffle = False,
                drop_last = False
            )
            
    
        if sampling_strategy == 'entropy':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                totpre, _ = get_output_emb(all_loader, classifier, False, args, model)
                # np.save(os.path.join(outpath, 'totpre' + str(alitr) + '.npy'), totpre)
                alidx += entropy_sampling(totpre, uidx, num_budget[alitr])
                uidx = list( set(uidx) - set(alidx) )
                
        elif sampling_strategy == 'random':
            if hyperalidx is not None:
                alidx = hyperalidx[:np.sum(num_budget[:alitr+1])]
                uidx = list( set(uidx) - set(alidx) )
            else:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            
        elif sampling_strategy == 'badge':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                emb = get_grad_embedding(classifier, all_loader, args, model)
                print(len(uidx), len(emb))
                newidx = kmeans_plus(uidx, emb, num_budget[alitr])
                alidx += newidx
                uidx = list( set(uidx) - set(alidx) )
                    
        elif sampling_strategy == 'badge_partition_subset':# set it small to no partition 
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                np.random.shuffle(uidx)
                candidx = uidx[:args.num_subset]
                
                num_partition = int(len(candidx) / args.num_partition)
                for ipart in range(args.num_partition):
                    print('badge partition ', ipart)
                    allset = FeasDataset( totfeas[candidx[num_partition*ipart:num_partition*(ipart+1)],:], totlabel[candidx[num_partition*ipart:num_partition*(ipart+1)]], None)
                    all_loader = torch.utils.data.DataLoader(
                        allset,
                        batch_size = args.batchsize_al_forward,
                        num_workers = 8,
                        shuffle = False,
                        drop_last = False
                    )
                    emb = get_grad_embedding(classifier, all_loader, args, model)
                    if num_budget[alitr] % args.num_partition != 0:
                        print('wrong partition setting')
                    newidx = kmeans_plus([i for i in range(len(emb))], emb, int(num_budget[alitr]/args.num_partition))
                    alidx += np.array(candidx[num_partition*ipart:num_partition*(ipart+1)])[newidx].tolist()
                    uidx = list( set(uidx) - set(alidx) )
                
        
        elif sampling_strategy == 'margin':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                #totpre, _ = get_output_emb(all_loader, classifier, False, model)
                #alidx += margin_sampling(totpre, uidx, num_budget[alitr])
                alidx += margin_sampling_model(uidx, all_loader, classifier, model, num_budget[alitr], args)
                uidx = list( set(uidx) - set(alidx) )
                
        elif sampling_strategy == 'confidence':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                alidx += confidence_sampling_model(uidx, all_loader, classifier, model, num_budget[alitr], args)
                uidx = list( set(uidx) - set(alidx) )
        
        elif sampling_strategy == 'ActiveFT(al)':
            if len(alidx) == 0:
                alidx = ActiveFT_sampling(totfeas, num_budget[alitr], alidx, args)
            else:
                _, alfeas = get_output_emb(all_loader, classifier, True, args, model)
                alidx = ActiveFT_sampling(alfeas, num_budget[alitr], alidx, args)
        
        elif sampling_strategy == 'ActiveFT(self)':
            if len(alidx) == 0:
                alidx = ActiveFT_sampling(totfeas, num_budget[alitr], alidx, args)
            else:
                alidx = alidx = ActiveFT_sampling(totfeas, num_budget[alitr], alidx, args)
        
        elif sampling_strategy == 'coreset':
            if len(alidx) == 0:
                np.random.shuffle(uidx)
                alidx += uidx[:num_budget[alitr]] 
                uidx = list( set(uidx) - set(alidx) )
            else:
                _, alfeas = get_output_emb(all_loader, classifier, True, args, model)
                np.save(os.path.join(outpath, 'alfeas' + str(alitr) + '.npy'), alfeas)
                label_feas = alfeas[alidx,:]
                uidx = list( set(uidx) - set(alidx) )
                unlabel_feas = alfeas[uidx,:]
                newidx = acquire_new_sample(num_budget[alitr], uidx, torch.from_numpy(label_feas), torch.from_numpy(unlabel_feas))
                alidx += newidx
        
        elif sampling_strategy == 'coreset_self':
            if len(alidx) == 0:
                alidx = [np.random.randint(0,len(allset))]
            label_feas = totfeas[alidx,:]
            uidx = list( set(uidx) - set(alidx) )
            unlabel_feas = totfeas[uidx,:]
            if len(alidx) == 1:
                newidx = acquire_new_sample(num_budget[alitr]-1, uidx, torch.from_numpy(label_feas), torch.from_numpy(unlabel_feas))
            else:
                newidx = acquire_new_sample(num_budget[alitr], uidx, torch.from_numpy(label_feas), torch.from_numpy(unlabel_feas))
            alidx += newidx

        
        np.save(os.path.join(outpath, 'alidx.npy'), np.array(alidx))
    
        print('point 1 sample selection', time.time() - s)
        s = time.time()
    
    if args.training_mode == 0:
        trainset = FeasDataset(totfeas[alidx,:], totlabel[alidx], std)
    else:
        trainset = get_dataset(args, transform_train, index = alidx, train = True )
    
    if len(trainset) < args.batchsize_train:
        Droplast = False
    else:
        Droplast = True
    train_loader = torch.utils.data.DataLoader(
        trainset,
        batch_size = args.batchsize_train,
        num_workers = args.num_workers,
        shuffle=True,
        #drop_last = False,#Droplast
    )
    
    ###class weight CE loss for class imbalance dataset
    if 'LTt' in args.dataset_name:# not that useful for freezemlp, just commend out
        lbltarget = trainset.targets
        class_weight = np.bincount(lbltarget, minlength=outdim_classifier )
        class_weight = 1./(class_weight+1)
        class_weight = class_weight / class_weight.sum()
        class_weight = torch.from_numpy(class_weight).float().cuda()
    else:
        class_weight = None
    
    if (args.load_al_weight and alitr == 0) or (not args.load_al_weight):
        if args.classifier_type == 'MLP':
            classifier = MLP(indim_classifier, hiddim_classifier, outdim_classifier)
        elif args.classifier_type == 'Linear':
            classifier = torch.nn.Linear(indim_classifier,outdim_classifier)
        else:
            raise NotImplementedError
    
    if args.load_proj_weight == 1:
        classifier.load_state_dict(model_dict)
    
    classifier.cuda()
    if args.distributed_training:
        classifier = torch.nn.DataParallel(classifier)
    
    if ( args.training_mode != 0 ) and ( 'clip' not in args.network ): 
        model = get_network(args)
        model.load_state_dict(encoder_dict)
        model.fc = torch.nn.Identity() 
        model.cuda()
        if args.distributed_training:
            model = torch.nn.DataParallel(model)
    
    print('point 2 model load', time.time() - s)
    s = time.time()
    
    ### training
    if args.training_mode == 0: 
        classifier, trainloss = train_mlp(train_loader, classifier, args, class_weight = class_weight)
        # torch.save({'epoch': 100, 'state_dict': classifier.state_dict()}, os.path.join(outpath, 'classifier' + str(len(alidx)) + '.pth'))
        torch.save({'epoch': args.train_eps, 'state_dict': classifier.state_dict()}, os.path.join(outpath, 'classifier.pth'))
    elif args.training_mode == 1:
        classifier, trainloss = train_freeze_mlp(train_loader, model, classifier, args, class_weight = class_weight)
        torch.save({'epoch': args.train_eps, 'classifier_state_dict': classifier.state_dict()}, os.path.join(outpath, 'checkpoint_' + str(len(alidx)) + '_.pth.tar'))
    elif args.training_mode == 2: 
        model, classifier, trainloss = train_fine_tune(train_loader, model, classifier, args, class_weight = class_weight)
        torch.save({'epoch': args.train_eps, 'classifier_state_dict': classifier.state_dict(), 'model_state_dict': model.state_dict()}, os.path.join(outpath, 'checkpoint_' + str(len(alidx)) + '_.pth.tar'))
    elif args.training_mode == 3:
        ### LP stage
        args.cls_lr = args.freezelr
        args.train_eps = args.freeze_eps
        classifier, trainloss = train_freeze_mlp(train_loader, model, classifier, args, class_weight = class_weight)# train_mlp(train_loader, classifier, args)
        print('trainloss freeze lp ', trainloss)
        tacc = evaluation(test_loader, classifier, model = model)
        torch.save({'acc': tacc, 'classifier_state_dict': classifier.state_dict()}, os.path.join(outpath, 'classifier_' + str(len(alidx)) + '.pth'))
        
        # FT stage
        args.lr = ftlr
        args.cls_lr = clslr
        args.train_eps = args.ft_eps
        model, classifier, trainloss = train_fine_tune(train_loader, model, classifier, args, class_weight = class_weight)
        print('trainloss ft ', trainloss)
        torch.save({'epoch': args.train_eps, 'classifier_state_dict': classifier.state_dict(), 'model_state_dict': model.state_dict()}, os.path.join(outpath, 'checkpoint_' + str(len(alidx)) + '_.pth.tar'))
        
    else:
        raise NotImplementedError
    
    print('point 3 training', time.time() - s)
    s = time.time()
    
    ### evaluation
    acc = evaluation(test_loader, classifier, model = model)
    tacc = evaluation(train_loader, classifier, model = model)
    
    print('point 4 evaluation', time.time() - s)
    s = time.time()
    
    #np.save(outpath + 'totpre' + str(len(alidx)) + '.npy', totpre)
    #np.save(outpath + 'alfeas' + str(len(alidx)) + '.npy', totemb)
    totacc += [acc]
    tracc += [tacc]
    print('AL lblset size is ', len(alidx), 'time ', time.time() - s)
    s = time.time()
    print('test acc: ', acc)
    print('train acc: ', tacc)
    np.save(os.path.join(outpath, 'acc.npy'), np.array(totacc))

### save
np.save(os.path.join(outpath, 'acc.npy'), np.array(totacc))

print('total time:', time.time() - s0)
